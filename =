
==> Audit <==
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMMAND   â”‚          ARGS          â”‚ PROFILE  â”‚ USER  â”‚ VERSION â”‚     START TIME      â”‚      END TIME       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ start      â”‚ - driver = docker      â”‚ minikube â”‚ rawat â”‚ v1.37.0 â”‚ 22 Nov 25 18:19 CET â”‚ 22 Nov 25 18:21 CET â”‚
â”‚ docker-env â”‚                        â”‚ minikube â”‚ rawat â”‚ v1.37.0 â”‚ 22 Nov 25 18:27 CET â”‚ 22 Nov 25 18:27 CET â”‚
â”‚ service    â”‚ ml-model-service --url â”‚ minikube â”‚ rawat â”‚ v1.37.0 â”‚ 22 Nov 25 18:31 CET â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


==> Last Start <==
Log file created at: 2025/11/22 18:19:34
Running on machine: Synamic
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1122 18:19:34.522684   91775 out.go:360] Setting OutFile to fd 1 ...
I1122 18:19:34.522914   91775 out.go:413] isatty.IsTerminal(1) = true
I1122 18:19:34.522918   91775 out.go:374] Setting ErrFile to fd 2...
I1122 18:19:34.522923   91775 out.go:413] isatty.IsTerminal(2) = true
I1122 18:19:34.523112   91775 root.go:338] Updating PATH: /home/rawat/.minikube/bin
W1122 18:19:34.523392   91775 root.go:314] Error reading config file at /home/rawat/.minikube/config/config.json: open /home/rawat/.minikube/config/config.json: no such file or directory
I1122 18:19:34.524011   91775 out.go:368] Setting JSON to false
I1122 18:19:34.525690   91775 start.go:130] hostinfo: {"hostname":"Synamic","uptime":8327,"bootTime":1763823647,"procs":353,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-85-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"8de1c2a0-411b-463b-95b6-c9f784c99913"}
I1122 18:19:34.525732   91775 start.go:140] virtualization: kvm host
I1122 18:19:34.529016   91775 out.go:179] ðŸ˜„  minikube v1.37.0 on Ubuntu 22.04
W1122 18:19:34.534609   91775 preload.go:293] Failed to list preload files: open /home/rawat/.minikube/cache/preloaded-tarball: no such file or directory
I1122 18:19:34.534704   91775 notify.go:220] Checking for updates...
I1122 18:19:34.534724   91775 driver.go:421] Setting default libvirt URI to qemu:///system
I1122 18:19:34.534742   91775 global.go:112] Querying for installed drivers using PATH=/home/rawat/.minikube/bin:/home/rawat/.nvm/versions/node/v22.11.0/bin:/home/rawat/.npm-global/bin:â€/home/rawat/.npm-packages/bin:/home/rawat/.local/bin:/home/rawat/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/binâ€
I1122 18:19:34.534761   91775 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1122 18:19:34.534926   91775 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1122 18:19:34.535054   91775 global.go:133] qemu2 default: true priority: 7, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1122 18:19:34.573610   91775 global.go:133] virtualbox default: true priority: 6, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:warning from virtualbox WARNING: The vboxdrv kernel module is not loaded. Either there is no module
         available for the current kernel (6.8.0-85-generic) or it failed to
         load. Please recompile the kernel module and install it by

           sudo /sbin/vboxconfig

         You will not be able to start VMs until this problem is fixed.
7.0.12r159484
 Reason: Fix:Read the docs for resolution Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1122 18:19:34.573682   91775 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1122 18:19:34.597212   91775 docker.go:123] docker version: linux-28.1.1:Docker Engine - Community
I1122 18:19:34.597289   91775 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1122 18:19:34.642875   91775 info.go:266] docker info: {ID:d999fc9b-41f0-451f-a59f-de6f84cb1830 Containers:11 ContainersRunning:0 ContainersPaused:0 ContainersStopped:11 Images:62 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-11-22 18:19:34.634202504 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-85-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16392024064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Synamic Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I1122 18:19:34.642932   91775 docker.go:318] overlay module found
I1122 18:19:34.642946   91775 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1122 18:19:34.651609   91775 global.go:133] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1122 18:19:34.651702   91775 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1122 18:19:34.651720   91775 driver.go:343] not recommending "ssh" due to default: false
I1122 18:19:34.651723   91775 driver.go:343] not recommending "none" due to default: false
I1122 18:19:34.651726   91775 driver.go:338] not recommending "virtualbox" due to health: warning from virtualbox WARNING: The vboxdrv kernel module is not loaded. Either there is no module
         available for the current kernel (6.8.0-85-generic) or it failed to
         load. Please recompile the kernel module and install it by

           sudo /sbin/vboxconfig

         You will not be able to start VMs until this problem is fixed.
7.0.12r159484
I1122 18:19:34.651743   91775 driver.go:378] Picked: docker
I1122 18:19:34.651748   91775 driver.go:379] Alternatives: [qemu2 ssh none]
I1122 18:19:34.651752   91775 driver.go:380] Rejects: [kvm2 virtualbox vmware podman]
I1122 18:19:34.654736   91775 out.go:179] âœ¨  Automatically selected the docker driver. Other choices: qemu2, ssh, none
I1122 18:19:34.657508   91775 start.go:304] selected driver: docker
I1122 18:19:34.657517   91775 start.go:918] validating driver "docker" against <nil>
I1122 18:19:34.657524   91775 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1122 18:19:34.657608   91775 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1122 18:19:34.706590   91775 info.go:266] docker info: {ID:d999fc9b-41f0-451f-a59f-de6f84cb1830 Containers:11 ContainersRunning:0 ContainersPaused:0 ContainersStopped:11 Images:62 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-11-22 18:19:34.698715534 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-85-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16392024064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Synamic Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I1122 18:19:34.706706   91775 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1122 18:19:34.707820   91775 start_flags.go:410] Using suggested 3900MB memory alloc based on sys=15632MB, container=15632MB
I1122 18:19:34.707989   91775 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1122 18:19:34.710717   91775 out.go:179] ðŸ“Œ  Using Docker driver with root privileges
I1122 18:19:34.713402   91775 cni.go:84] Creating CNI manager for ""
I1122 18:19:34.713452   91775 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1122 18:19:34.713457   91775 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1122 18:19:34.713510   91775 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1122 18:19:34.716594   91775 out.go:179] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I1122 18:19:34.719382   91775 cache.go:123] Beginning downloading kic base image for docker with docker
I1122 18:19:34.725390   91775 out.go:179] ðŸšœ  Pulling base image v0.0.48 ...
I1122 18:19:34.731437   91775 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1122 18:19:34.731549   91775 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1122 18:19:34.803620   91775 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1122 18:19:34.803781   91775 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1122 18:19:34.803901   91775 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1122 18:19:34.858357   91775 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1122 18:19:34.858389   91775 cache.go:58] Caching tarball of preloaded images
I1122 18:19:34.858703   91775 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1122 18:19:34.865747   91775 out.go:179] ðŸ’¾  Downloading Kubernetes v1.34.0 preload ...
I1122 18:19:34.868798   91775 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1122 18:19:35.066202   91775 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/rawat/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1122 18:20:33.157022   91775 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1122 18:20:33.157047   91775 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1122 18:20:46.898959   91775 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1122 18:20:47.574328   91775 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1122 18:20:47.574404   91775 preload.go:254] verifying checksum of /home/rawat/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1122 18:20:48.107019   91775 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1122 18:20:48.107222   91775 profile.go:143] Saving config to /home/rawat/.minikube/profiles/minikube/config.json ...
I1122 18:20:48.107236   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/profiles/minikube/config.json: {Name:mk8850d471dddfa8cb4e8f1a3aa942b4a2016964 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:20:48.107341   91775 cache.go:232] Successfully downloaded all kic artifacts
I1122 18:20:48.107350   91775 start.go:360] acquireMachinesLock for minikube: {Name:mk46275bce86b8795b40728634e8b015a7f2ac37 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1122 18:20:48.107380   91775 start.go:364] duration metric: took 23.91Âµs to acquireMachinesLock for "minikube"
I1122 18:20:48.107390   91775 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1122 18:20:48.107421   91775 start.go:125] createHost starting for "" (driver="docker")
I1122 18:20:48.117660   91775 out.go:252] ðŸ”¥  Creating docker container (CPUs=2, Memory=3900MB) ...
I1122 18:20:48.117877   91775 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1122 18:20:48.117896   91775 client.go:168] LocalClient.Create starting
I1122 18:20:48.118046   91775 main.go:141] libmachine: Creating CA: /home/rawat/.minikube/certs/ca.pem
I1122 18:20:48.200685   91775 main.go:141] libmachine: Creating client certificate: /home/rawat/.minikube/certs/cert.pem
I1122 18:20:48.552839   91775 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1122 18:20:48.567297   91775 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1122 18:20:48.567357   91775 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1122 18:20:48.567369   91775 cli_runner.go:164] Run: docker network inspect minikube
W1122 18:20:48.581207   91775 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1122 18:20:48.581226   91775 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1122 18:20:48.581238   91775 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1122 18:20:48.581380   91775 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1122 18:20:48.595896   91775 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc00211eaa0}
I1122 18:20:48.595924   91775 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1122 18:20:48.595967   91775 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1122 18:20:48.666312   91775 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1122 18:20:48.666339   91775 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1122 18:20:48.666452   91775 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1122 18:20:48.685287   91775 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1122 18:20:48.704265   91775 oci.go:103] Successfully created a docker volume minikube
I1122 18:20:48.704339   91775 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1122 18:20:49.352697   91775 oci.go:107] Successfully prepared a docker volume minikube
I1122 18:20:49.352721   91775 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1122 18:20:49.352739   91775 kic.go:194] Starting extracting preloaded images to volume ...
I1122 18:20:49.352811   91775 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/rawat/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1122 18:20:51.502052   91775 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/rawat/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (2.149179531s)
I1122 18:20:51.502092   91775 kic.go:203] duration metric: took 2.149341983s to extract preloaded images to volume ...
W1122 18:20:51.502359   91775 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1122 18:20:51.502480   91775 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1122 18:20:51.502602   91775 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1122 18:20:51.597261   91775 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1122 18:20:51.964428   91775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1122 18:20:51.982346   91775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1122 18:20:52.001084   91775 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1122 18:20:52.076070   91775 oci.go:144] the created container "minikube" has a running status.
I1122 18:20:52.076112   91775 kic.go:225] Creating ssh key for kic: /home/rawat/.minikube/machines/minikube/id_rsa...
I1122 18:20:52.479953   91775 kic_runner.go:191] docker (temp): /home/rawat/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1122 18:20:52.509834   91775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1122 18:20:52.526434   91775 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1122 18:20:52.526441   91775 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1122 18:20:52.580211   91775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1122 18:20:52.596822   91775 machine.go:93] provisionDockerMachine start ...
I1122 18:20:52.596891   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:52.614443   91775 main.go:141] libmachine: Using SSH client type: native
I1122 18:20:52.614644   91775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1122 18:20:52.614650   91775 main.go:141] libmachine: About to run SSH command:
hostname
I1122 18:20:52.767105   91775 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1122 18:20:52.767191   91775 ubuntu.go:182] provisioning hostname "minikube"
I1122 18:20:52.767468   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:52.808178   91775 main.go:141] libmachine: Using SSH client type: native
I1122 18:20:52.808421   91775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1122 18:20:52.808430   91775 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1122 18:20:53.010925   91775 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1122 18:20:53.011112   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:53.057109   91775 main.go:141] libmachine: Using SSH client type: native
I1122 18:20:53.057631   91775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1122 18:20:53.057660   91775 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1122 18:20:53.255254   91775 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1122 18:20:53.255300   91775 ubuntu.go:188] set auth options {CertDir:/home/rawat/.minikube CaCertPath:/home/rawat/.minikube/certs/ca.pem CaPrivateKeyPath:/home/rawat/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/rawat/.minikube/machines/server.pem ServerKeyPath:/home/rawat/.minikube/machines/server-key.pem ClientKeyPath:/home/rawat/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/rawat/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/rawat/.minikube}
I1122 18:20:53.255381   91775 ubuntu.go:190] setting up certificates
I1122 18:20:53.255407   91775 provision.go:84] configureAuth start
I1122 18:20:53.255608   91775 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1122 18:20:53.299766   91775 provision.go:143] copyHostCerts
I1122 18:20:53.299841   91775 exec_runner.go:151] cp: /home/rawat/.minikube/certs/ca.pem --> /home/rawat/.minikube/ca.pem (1074 bytes)
I1122 18:20:53.299990   91775 exec_runner.go:151] cp: /home/rawat/.minikube/certs/cert.pem --> /home/rawat/.minikube/cert.pem (1119 bytes)
I1122 18:20:53.300096   91775 exec_runner.go:151] cp: /home/rawat/.minikube/certs/key.pem --> /home/rawat/.minikube/key.pem (1679 bytes)
I1122 18:20:53.300202   91775 provision.go:117] generating server cert: /home/rawat/.minikube/machines/server.pem ca-key=/home/rawat/.minikube/certs/ca.pem private-key=/home/rawat/.minikube/certs/ca-key.pem org=rawat.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1122 18:20:53.467217   91775 provision.go:177] copyRemoteCerts
I1122 18:20:53.467308   91775 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1122 18:20:53.467390   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:53.483447   91775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rawat/.minikube/machines/minikube/id_rsa Username:docker}
I1122 18:20:53.604240   91775 ssh_runner.go:362] scp /home/rawat/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1122 18:20:53.709006   91775 ssh_runner.go:362] scp /home/rawat/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I1122 18:20:53.776150   91775 ssh_runner.go:362] scp /home/rawat/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1122 18:20:53.815070   91775 provision.go:87] duration metric: took 559.64967ms to configureAuth
I1122 18:20:53.815087   91775 ubuntu.go:206] setting minikube options for container-runtime
I1122 18:20:53.815265   91775 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1122 18:20:53.815340   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:53.839822   91775 main.go:141] libmachine: Using SSH client type: native
I1122 18:20:53.840102   91775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1122 18:20:53.840111   91775 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1122 18:20:54.028676   91775 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1122 18:20:54.028708   91775 ubuntu.go:71] root file system type: overlay
I1122 18:20:54.028952   91775 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1122 18:20:54.029128   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:54.072400   91775 main.go:141] libmachine: Using SSH client type: native
I1122 18:20:54.072679   91775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1122 18:20:54.072777   91775 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1122 18:20:54.310662   91775 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1122 18:20:54.310838   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:54.356371   91775 main.go:141] libmachine: Using SSH client type: native
I1122 18:20:54.356703   91775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1122 18:20:54.356726   91775 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1122 18:20:56.494484   91775 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-11-22 17:20:54.305215417 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1122 18:20:56.494503   91775 machine.go:96] duration metric: took 3.897672069s to provisionDockerMachine
I1122 18:20:56.494520   91775 client.go:171] duration metric: took 8.376611344s to LocalClient.Create
I1122 18:20:56.494535   91775 start.go:167] duration metric: took 8.376658735s to libmachine.API.Create "minikube"
I1122 18:20:56.494541   91775 start.go:293] postStartSetup for "minikube" (driver="docker")
I1122 18:20:56.494549   91775 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1122 18:20:56.494632   91775 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1122 18:20:56.494686   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:56.512988   91775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rawat/.minikube/machines/minikube/id_rsa Username:docker}
I1122 18:20:56.648552   91775 ssh_runner.go:195] Run: cat /etc/os-release
I1122 18:20:56.659870   91775 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1122 18:20:56.659985   91775 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1122 18:20:56.660024   91775 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1122 18:20:56.660038   91775 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1122 18:20:56.660057   91775 filesync.go:126] Scanning /home/rawat/.minikube/addons for local assets ...
I1122 18:20:56.660213   91775 filesync.go:126] Scanning /home/rawat/.minikube/files for local assets ...
I1122 18:20:56.660293   91775 start.go:296] duration metric: took 165.743844ms for postStartSetup
I1122 18:20:56.661138   91775 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1122 18:20:56.692975   91775 profile.go:143] Saving config to /home/rawat/.minikube/profiles/minikube/config.json ...
I1122 18:20:56.693247   91775 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1122 18:20:56.693287   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:56.710369   91775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rawat/.minikube/machines/minikube/id_rsa Username:docker}
I1122 18:20:56.810577   91775 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1122 18:20:56.825840   91775 start.go:128] duration metric: took 8.718393982s to createHost
I1122 18:20:56.825865   91775 start.go:83] releasing machines lock for "minikube", held for 8.718475053s
I1122 18:20:56.826082   91775 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1122 18:20:56.860602   91775 ssh_runner.go:195] Run: cat /version.json
I1122 18:20:56.860650   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:56.860672   91775 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1122 18:20:56.860726   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:20:56.880758   91775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rawat/.minikube/machines/minikube/id_rsa Username:docker}
I1122 18:20:56.881079   91775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rawat/.minikube/machines/minikube/id_rsa Username:docker}
I1122 18:20:57.226081   91775 ssh_runner.go:195] Run: systemctl --version
I1122 18:20:57.231192   91775 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1122 18:20:57.236643   91775 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1122 18:20:57.327395   91775 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1122 18:20:57.327568   91775 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1122 18:20:57.429149   91775 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I1122 18:20:57.429183   91775 start.go:495] detecting cgroup driver to use...
I1122 18:20:57.429250   91775 detect.go:190] detected "systemd" cgroup driver on host os
I1122 18:20:57.429529   91775 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1122 18:20:57.489905   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1122 18:20:57.506996   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1122 18:20:57.520211   91775 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1122 18:20:57.520278   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1122 18:20:57.533699   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1122 18:20:57.546521   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1122 18:20:57.559178   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1122 18:20:57.571761   91775 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1122 18:20:57.584525   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1122 18:20:57.599584   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1122 18:20:57.612857   91775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1122 18:20:57.625851   91775 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1122 18:20:57.637365   91775 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1122 18:20:57.637428   91775 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1122 18:20:57.653781   91775 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1122 18:20:57.665058   91775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 18:20:57.740920   91775 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1122 18:20:57.827251   91775 start.go:495] detecting cgroup driver to use...
I1122 18:20:57.827276   91775 detect.go:190] detected "systemd" cgroup driver on host os
I1122 18:20:57.827328   91775 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1122 18:20:57.839946   91775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1122 18:20:57.851438   91775 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1122 18:20:57.866760   91775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1122 18:20:57.878504   91775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1122 18:20:57.893738   91775 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1122 18:20:57.911349   91775 ssh_runner.go:195] Run: which cri-dockerd
I1122 18:20:57.914869   91775 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1122 18:20:57.927327   91775 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1122 18:20:57.945988   91775 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1122 18:20:58.021445   91775 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1122 18:20:58.094212   91775 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1122 18:20:58.094277   91775 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1122 18:20:58.112482   91775 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1122 18:20:58.123146   91775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 18:20:58.193735   91775 ssh_runner.go:195] Run: sudo systemctl restart docker
I1122 18:20:59.828678   91775 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.634888758s)
I1122 18:20:59.828878   91775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1122 18:20:59.860299   91775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1122 18:20:59.878849   91775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1122 18:20:59.898642   91775 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1122 18:20:59.998579   91775 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1122 18:21:00.077968   91775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 18:21:00.150706   91775 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1122 18:21:00.170597   91775 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1122 18:21:00.182306   91775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 18:21:00.253984   91775 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1122 18:21:00.326895   91775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1122 18:21:00.340693   91775 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1122 18:21:00.340781   91775 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1122 18:21:00.344919   91775 start.go:563] Will wait 60s for crictl version
I1122 18:21:00.344965   91775 ssh_runner.go:195] Run: which crictl
I1122 18:21:00.348804   91775 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1122 18:21:00.386709   91775 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1122 18:21:00.386764   91775 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1122 18:21:00.417047   91775 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1122 18:21:00.456664   91775 out.go:252] ðŸ³  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1122 18:21:00.456905   91775 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1122 18:21:00.503020   91775 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1122 18:21:00.508827   91775 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1122 18:21:00.521953   91775 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1122 18:21:00.522046   91775 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1122 18:21:00.522109   91775 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1122 18:21:00.542470   91775 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1122 18:21:00.542477   91775 docker.go:621] Images already preloaded, skipping extraction
I1122 18:21:00.542571   91775 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1122 18:21:00.561370   91775 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1122 18:21:00.561379   91775 cache_images.go:85] Images are preloaded, skipping loading
I1122 18:21:00.561385   91775 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1122 18:21:00.561445   91775 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1122 18:21:00.561492   91775 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1122 18:21:00.607525   91775 cni.go:84] Creating CNI manager for ""
I1122 18:21:00.607539   91775 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1122 18:21:00.607548   91775 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1122 18:21:00.607570   91775 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1122 18:21:00.607686   91775 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1122 18:21:00.607749   91775 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1122 18:21:00.618327   91775 binaries.go:44] Found k8s binaries, skipping transfer
I1122 18:21:00.618443   91775 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1122 18:21:00.629151   91775 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1122 18:21:00.648336   91775 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1122 18:21:00.667507   91775 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1122 18:21:00.686418   91775 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1122 18:21:00.690142   91775 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1122 18:21:00.702419   91775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 18:21:00.776905   91775 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1122 18:21:00.809917   91775 certs.go:68] Setting up /home/rawat/.minikube/profiles/minikube for IP: 192.168.49.2
I1122 18:21:00.809924   91775 certs.go:194] generating shared ca certs ...
I1122 18:21:00.809935   91775 certs.go:226] acquiring lock for ca certs: {Name:mk7286868966ecba1115b1046b84e4fc47b1c822 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:00.810058   91775 certs.go:240] generating "minikubeCA" ca cert: /home/rawat/.minikube/ca.key
I1122 18:21:00.844988   91775 crypto.go:156] Writing cert to /home/rawat/.minikube/ca.crt ...
I1122 18:21:00.844997   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/ca.crt: {Name:mk013c2f6a2fa47b6e47d9f01585a27e3e60ce3e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:00.845152   91775 crypto.go:164] Writing key to /home/rawat/.minikube/ca.key ...
I1122 18:21:00.845157   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/ca.key: {Name:mk658cfa48358122b0c107e772c365b795011d3f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:00.845240   91775 certs.go:240] generating "proxyClientCA" ca cert: /home/rawat/.minikube/proxy-client-ca.key
I1122 18:21:01.157498   91775 crypto.go:156] Writing cert to /home/rawat/.minikube/proxy-client-ca.crt ...
I1122 18:21:01.157508   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/proxy-client-ca.crt: {Name:mkaa17d997864936e4bfffe6978890b35d5bcdbd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.157642   91775 crypto.go:164] Writing key to /home/rawat/.minikube/proxy-client-ca.key ...
I1122 18:21:01.157647   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/proxy-client-ca.key: {Name:mk4fe0599d2d66142ebbb5fec43c4f71a3bf2729 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.157723   91775 certs.go:256] generating profile certs ...
I1122 18:21:01.157766   91775 certs.go:363] generating signed profile cert for "minikube-user": /home/rawat/.minikube/profiles/minikube/client.key
I1122 18:21:01.157776   91775 crypto.go:68] Generating cert /home/rawat/.minikube/profiles/minikube/client.crt with IP's: []
I1122 18:21:01.481357   91775 crypto.go:156] Writing cert to /home/rawat/.minikube/profiles/minikube/client.crt ...
I1122 18:21:01.481373   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/profiles/minikube/client.crt: {Name:mk553fe2d5c6deb5f8c7de9d6bd603cbcc667336 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.481503   91775 crypto.go:164] Writing key to /home/rawat/.minikube/profiles/minikube/client.key ...
I1122 18:21:01.481508   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/profiles/minikube/client.key: {Name:mke3bb07609275cefdc536a2e0382de83370998f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.481587   91775 certs.go:363] generating signed profile cert for "minikube": /home/rawat/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1122 18:21:01.481602   91775 crypto.go:68] Generating cert /home/rawat/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1122 18:21:01.586999   91775 crypto.go:156] Writing cert to /home/rawat/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1122 18:21:01.587009   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk2735bd9f651ecdf69e4245823ef1ea151be736 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.587146   91775 crypto.go:164] Writing key to /home/rawat/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1122 18:21:01.587151   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkb9ba1e848446dbc087b4a9a76f5d5e9122c507 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.587236   91775 certs.go:381] copying /home/rawat/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/rawat/.minikube/profiles/minikube/apiserver.crt
I1122 18:21:01.587339   91775 certs.go:385] copying /home/rawat/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/rawat/.minikube/profiles/minikube/apiserver.key
I1122 18:21:01.587410   91775 certs.go:363] generating signed profile cert for "aggregator": /home/rawat/.minikube/profiles/minikube/proxy-client.key
I1122 18:21:01.587426   91775 crypto.go:68] Generating cert /home/rawat/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1122 18:21:01.674570   91775 crypto.go:156] Writing cert to /home/rawat/.minikube/profiles/minikube/proxy-client.crt ...
I1122 18:21:01.674582   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/profiles/minikube/proxy-client.crt: {Name:mk574cb487eb3869ac68e623bda0c8bce7aa9013 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.674716   91775 crypto.go:164] Writing key to /home/rawat/.minikube/profiles/minikube/proxy-client.key ...
I1122 18:21:01.674721   91775 lock.go:35] WriteFile acquiring /home/rawat/.minikube/profiles/minikube/proxy-client.key: {Name:mkfabb8ee4a4639e146b7fc69553ce46e2c7bc94 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:01.674941   91775 certs.go:484] found cert: /home/rawat/.minikube/certs/ca-key.pem (1675 bytes)
I1122 18:21:01.674974   91775 certs.go:484] found cert: /home/rawat/.minikube/certs/ca.pem (1074 bytes)
I1122 18:21:01.674996   91775 certs.go:484] found cert: /home/rawat/.minikube/certs/cert.pem (1119 bytes)
I1122 18:21:01.675023   91775 certs.go:484] found cert: /home/rawat/.minikube/certs/key.pem (1679 bytes)
I1122 18:21:01.675523   91775 ssh_runner.go:362] scp /home/rawat/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1122 18:21:01.697347   91775 ssh_runner.go:362] scp /home/rawat/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1122 18:21:01.721376   91775 ssh_runner.go:362] scp /home/rawat/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1122 18:21:01.746978   91775 ssh_runner.go:362] scp /home/rawat/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1122 18:21:01.774128   91775 ssh_runner.go:362] scp /home/rawat/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1122 18:21:01.804334   91775 ssh_runner.go:362] scp /home/rawat/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1122 18:21:01.840902   91775 ssh_runner.go:362] scp /home/rawat/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1122 18:21:01.875214   91775 ssh_runner.go:362] scp /home/rawat/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1122 18:21:01.910256   91775 ssh_runner.go:362] scp /home/rawat/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1122 18:21:01.978802   91775 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1122 18:21:02.040452   91775 ssh_runner.go:195] Run: openssl version
I1122 18:21:02.050477   91775 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1122 18:21:02.083157   91775 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1122 18:21:02.096245   91775 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 22 17:21 /usr/share/ca-certificates/minikubeCA.pem
I1122 18:21:02.096408   91775 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1122 18:21:02.123178   91775 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1122 18:21:02.156612   91775 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1122 18:21:02.165178   91775 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1122 18:21:02.165256   91775 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1122 18:21:02.165498   91775 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1122 18:21:02.192020   91775 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1122 18:21:02.200839   91775 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1122 18:21:02.213587   91775 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1122 18:21:02.213675   91775 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1122 18:21:02.229285   91775 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1122 18:21:02.229358   91775 kubeadm.go:157] found existing configuration files:

I1122 18:21:02.229466   91775 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1122 18:21:02.243546   91775 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1122 18:21:02.243611   91775 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1122 18:21:02.255796   91775 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1122 18:21:02.265810   91775 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1122 18:21:02.265882   91775 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1122 18:21:02.274946   91775 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1122 18:21:02.282605   91775 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1122 18:21:02.282658   91775 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1122 18:21:02.290136   91775 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1122 18:21:02.298058   91775 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1122 18:21:02.298112   91775 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1122 18:21:02.306595   91775 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1122 18:21:02.342361   91775 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1122 18:21:02.342491   91775 kubeadm.go:310] [preflight] Running pre-flight checks
I1122 18:21:02.362447   91775 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I1122 18:21:02.362531   91775 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.8.0-85-generic[0m
I1122 18:21:02.362578   91775 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I1122 18:21:02.362641   91775 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I1122 18:21:02.362718   91775 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I1122 18:21:02.362780   91775 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I1122 18:21:02.362841   91775 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I1122 18:21:02.362903   91775 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I1122 18:21:02.362947   91775 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I1122 18:21:02.362988   91775 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I1122 18:21:02.363055   91775 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I1122 18:21:02.416429   91775 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1122 18:21:02.416683   91775 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1122 18:21:02.416907   91775 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1122 18:21:02.426090   91775 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1122 18:21:02.438710   91775 out.go:252]     â–ª Generating certificates and keys ...
I1122 18:21:02.438788   91775 kubeadm.go:310] [certs] Using existing ca certificate authority
I1122 18:21:02.438866   91775 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1122 18:21:02.653045   91775 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1122 18:21:03.388725   91775 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1122 18:21:03.661757   91775 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1122 18:21:03.859795   91775 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1122 18:21:03.950471   91775 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1122 18:21:03.950583   91775 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1122 18:21:04.150815   91775 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1122 18:21:04.150929   91775 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1122 18:21:05.073143   91775 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1122 18:21:05.112081   91775 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1122 18:21:05.669608   91775 kubeadm.go:310] [certs] Generating "sa" key and public key
I1122 18:21:05.669742   91775 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1122 18:21:06.136589   91775 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1122 18:21:06.325713   91775 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1122 18:21:06.476496   91775 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1122 18:21:06.571396   91775 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1122 18:21:06.941223   91775 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1122 18:21:06.941958   91775 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1122 18:21:06.949573   91775 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1122 18:21:06.952820   91775 out.go:252]     â–ª Booting up control plane ...
I1122 18:21:06.952993   91775 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1122 18:21:06.953156   91775 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1122 18:21:06.953343   91775 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1122 18:21:06.968044   91775 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1122 18:21:06.968180   91775 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1122 18:21:06.977682   91775 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1122 18:21:06.977986   91775 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1122 18:21:06.978031   91775 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1122 18:21:07.080514   91775 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1122 18:21:07.080760   91775 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1122 18:21:07.581334   91775 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.116012ms
I1122 18:21:07.585325   91775 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1122 18:21:07.585468   91775 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1122 18:21:07.585583   91775 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1122 18:21:07.585696   91775 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1122 18:21:09.977813   91775 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 2.392551206s
I1122 18:21:10.617129   91775 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 3.031830174s
I1122 18:21:13.091239   91775 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 5.505207522s
I1122 18:21:13.134402   91775 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1122 18:21:13.159014   91775 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1122 18:21:13.184143   91775 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1122 18:21:13.184925   91775 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1122 18:21:13.212653   91775 kubeadm.go:310] [bootstrap-token] Using token: v50nbs.3fb6zyqiphyu1ho9
I1122 18:21:13.215804   91775 out.go:252]     â–ª Configuring RBAC rules ...
I1122 18:21:13.216360   91775 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1122 18:21:13.226689   91775 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1122 18:21:13.240050   91775 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1122 18:21:13.245411   91775 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1122 18:21:13.249580   91775 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1122 18:21:13.254828   91775 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1122 18:21:13.512953   91775 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1122 18:21:13.955501   91775 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1122 18:21:14.506225   91775 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1122 18:21:14.509388   91775 kubeadm.go:310] 
I1122 18:21:14.509661   91775 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1122 18:21:14.509684   91775 kubeadm.go:310] 
I1122 18:21:14.510186   91775 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1122 18:21:14.510210   91775 kubeadm.go:310] 
I1122 18:21:14.510397   91775 kubeadm.go:310]   mkdir -p $HOME/.kube
I1122 18:21:14.510652   91775 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1122 18:21:14.510922   91775 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1122 18:21:14.510940   91775 kubeadm.go:310] 
I1122 18:21:14.511228   91775 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1122 18:21:14.511245   91775 kubeadm.go:310] 
I1122 18:21:14.511439   91775 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1122 18:21:14.511457   91775 kubeadm.go:310] 
I1122 18:21:14.511629   91775 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1122 18:21:14.512039   91775 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1122 18:21:14.512466   91775 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1122 18:21:14.512485   91775 kubeadm.go:310] 
I1122 18:21:14.513016   91775 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1122 18:21:14.513450   91775 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1122 18:21:14.513466   91775 kubeadm.go:310] 
I1122 18:21:14.513941   91775 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token v50nbs.3fb6zyqiphyu1ho9 \
I1122 18:21:14.514598   91775 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:12f1394b06bcda8f198a992c397fa9c01b6192e90aecb81c07a3e63fd2a53135 \
I1122 18:21:14.514684   91775 kubeadm.go:310] 	--control-plane 
I1122 18:21:14.514694   91775 kubeadm.go:310] 
I1122 18:21:14.515157   91775 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1122 18:21:14.515225   91775 kubeadm.go:310] 
I1122 18:21:14.515800   91775 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token v50nbs.3fb6zyqiphyu1ho9 \
I1122 18:21:14.516310   91775 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:12f1394b06bcda8f198a992c397fa9c01b6192e90aecb81c07a3e63fd2a53135 
I1122 18:21:14.522752   91775 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1122 18:21:14.524128   91775 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.8.0-85-generic\n", err: exit status 1
I1122 18:21:14.524630   91775 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1122 18:21:14.524661   91775 cni.go:84] Creating CNI manager for ""
I1122 18:21:14.524690   91775 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1122 18:21:14.529577   91775 out.go:179] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I1122 18:21:14.532901   91775 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1122 18:21:14.563832   91775 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1122 18:21:14.588545   91775 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1122 18:21:14.588665   91775 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1122 18:21:14.588697   91775 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_11_22T18_21_14_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1122 18:21:14.670309   91775 kubeadm.go:1105] duration metric: took 81.766977ms to wait for elevateKubeSystemPrivileges
I1122 18:21:14.670371   91775 ops.go:34] apiserver oom_adj: -16
I1122 18:21:14.670389   91775 kubeadm.go:394] duration metric: took 12.50514546s to StartCluster
I1122 18:21:14.670402   91775 settings.go:142] acquiring lock: {Name:mk59d52a1367886ae50077a12713efaa2af2b159 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:14.670487   91775 settings.go:150] Updating kubeconfig:  /home/rawat/.kube/config
I1122 18:21:14.670910   91775 lock.go:35] WriteFile acquiring /home/rawat/.kube/config: {Name:mk010ceffdba789da73ae2941d59cf47277e56ad Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 18:21:14.671062   91775 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1122 18:21:14.671082   91775 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1122 18:21:14.671177   91775 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1122 18:21:14.671256   91775 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1122 18:21:14.671266   91775 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1122 18:21:14.671268   91775 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1122 18:21:14.671279   91775 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1122 18:21:14.671286   91775 host.go:66] Checking if "minikube" exists ...
I1122 18:21:14.671300   91775 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1122 18:21:14.671521   91775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1122 18:21:14.671620   91775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1122 18:21:14.673885   91775 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I1122 18:21:14.679245   91775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 18:21:14.690762   91775 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1122 18:21:14.690784   91775 host.go:66] Checking if "minikube" exists ...
I1122 18:21:14.691500   91775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1122 18:21:14.692214   91775 out.go:179]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1122 18:21:14.695231   91775 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1122 18:21:14.695243   91775 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1122 18:21:14.695365   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:21:14.716347   91775 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1122 18:21:14.716360   91775 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1122 18:21:14.716423   91775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1122 18:21:14.723879   91775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rawat/.minikube/machines/minikube/id_rsa Username:docker}
I1122 18:21:14.752817   91775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rawat/.minikube/machines/minikube/id_rsa Username:docker}
I1122 18:21:14.789973   91775 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1122 18:21:14.857882   91775 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1122 18:21:14.869436   91775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1122 18:21:14.884497   91775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1122 18:21:15.018016   91775 start.go:976] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1122 18:21:15.018753   91775 api_server.go:52] waiting for apiserver process to appear ...
I1122 18:21:15.018805   91775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 18:21:15.173451   91775 api_server.go:72] duration metric: took 502.329393ms to wait for apiserver process to appear ...
I1122 18:21:15.173460   91775 api_server.go:88] waiting for apiserver healthz status ...
I1122 18:21:15.173471   91775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1122 18:21:15.177334   91775 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1122 18:21:15.178113   91775 api_server.go:141] control plane version: v1.34.0
I1122 18:21:15.178124   91775 api_server.go:131] duration metric: took 4.658958ms to wait for apiserver health ...
I1122 18:21:15.178130   91775 system_pods.go:43] waiting for kube-system pods to appear ...
I1122 18:21:15.179775   91775 system_pods.go:59] 5 kube-system pods found
I1122 18:21:15.179786   91775 system_pods.go:61] "etcd-minikube" [5a6470aa-132b-41f7-89a0-91fe0c455e4c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1122 18:21:15.179791   91775 system_pods.go:61] "kube-apiserver-minikube" [6157f659-733f-482e-941d-19665f463268] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1122 18:21:15.179796   91775 system_pods.go:61] "kube-controller-manager-minikube" [8d2e3745-5bac-488c-b276-16974b6d808d] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1122 18:21:15.179799   91775 system_pods.go:61] "kube-scheduler-minikube" [4cb723a6-3fd6-4eab-84ac-73b5a46b006e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1122 18:21:15.179803   91775 system_pods.go:61] "storage-provisioner" [6fd1be25-ef9a-43c4-8f61-2ef32b5b3ae4] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I1122 18:21:15.179806   91775 system_pods.go:74] duration metric: took 1.673599ms to wait for pod list to return data ...
I1122 18:21:15.179812   91775 kubeadm.go:578] duration metric: took 508.692717ms to wait for: map[apiserver:true system_pods:true]
I1122 18:21:15.179819   91775 node_conditions.go:102] verifying NodePressure condition ...
I1122 18:21:15.180804   91775 out.go:179] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I1122 18:21:15.181257   91775 node_conditions.go:122] node storage ephemeral capacity is 490617784Ki
I1122 18:21:15.181276   91775 node_conditions.go:123] node cpu capacity is 12
I1122 18:21:15.181288   91775 node_conditions.go:105] duration metric: took 1.466305ms to run NodePressure ...
I1122 18:21:15.181295   91775 start.go:241] waiting for startup goroutines ...
I1122 18:21:15.187897   91775 addons.go:514] duration metric: took 516.724197ms for enable addons: enabled=[storage-provisioner default-storageclass]
I1122 18:21:15.524284   91775 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1122 18:21:15.524381   91775 start.go:246] waiting for cluster config update ...
I1122 18:21:15.524409   91775 start.go:255] writing updated cluster config ...
I1122 18:21:15.525167   91775 ssh_runner.go:195] Run: rm -f paused
I1122 18:21:15.615219   91775 start.go:617] kubectl: 1.31.0, cluster: 1.34.0 (minor skew: 3)
I1122 18:21:15.618019   91775 out.go:203] 
W1122 18:21:15.620734   91775 out.go:285] â—  /usr/local/bin/kubectl is version 1.31.0, which may have incompatibilities with Kubernetes 1.34.0.
I1122 18:21:15.623581   91775 out.go:179]     â–ª Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I1122 18:21:15.629482   91775 out.go:179] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 22 17:20:56 minikube dockerd[689]: time="2025-11-22T17:20:56.491711018Z" level=info msg="API listen on /var/run/docker.sock"
Nov 22 17:20:56 minikube dockerd[689]: time="2025-11-22T17:20:56.491730923Z" level=info msg="API listen on [::]:2376"
Nov 22 17:20:56 minikube systemd[1]: Started Docker Application Container Engine.
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.750764364Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.750800503Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.750769011Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.750916698Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.862101718Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.862133944Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.862191769Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Nov 22 17:20:57 minikube dockerd[689]: time="2025-11-22T17:20:57.862222540Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Nov 22 17:20:58 minikube systemd[1]: Stopping Docker Application Container Engine...
Nov 22 17:20:58 minikube dockerd[689]: time="2025-11-22T17:20:58.204256425Z" level=info msg="Processing signal 'terminated'"
Nov 22 17:20:58 minikube dockerd[689]: time="2025-11-22T17:20:58.205207663Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: grpc: the client connection is closing"
Nov 22 17:20:58 minikube dockerd[689]: time="2025-11-22T17:20:58.205497814Z" level=info msg="Daemon shutdown complete"
Nov 22 17:20:58 minikube dockerd[689]: time="2025-11-22T17:20:58.205541094Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: context canceled while waiting for connections to become ready"
Nov 22 17:20:58 minikube systemd[1]: docker.service: Deactivated successfully.
Nov 22 17:20:58 minikube systemd[1]: Stopped Docker Application Container Engine.
Nov 22 17:20:58 minikube systemd[1]: docker.service: Consumed 1.141s CPU time.
Nov 22 17:20:58 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 22 17:20:58 minikube dockerd[1140]: time="2025-11-22T17:20:58.327161761Z" level=info msg="Starting up"
Nov 22 17:20:58 minikube dockerd[1140]: time="2025-11-22T17:20:58.327886299Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Nov 22 17:20:58 minikube dockerd[1140]: time="2025-11-22T17:20:58.327961787Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Nov 22 17:20:58 minikube dockerd[1140]: time="2025-11-22T17:20:58.327980252Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Nov 22 17:20:58 minikube dockerd[1140]: time="2025-11-22T17:20:58.337028589Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Nov 22 17:20:58 minikube dockerd[1140]: time="2025-11-22T17:20:58.349452227Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 22 17:20:58 minikube dockerd[1140]: time="2025-11-22T17:20:58.403459304Z" level=info msg="Loading containers: start."
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.539459484Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count bb0e7c8ff479cfeceab13d7b10f001316bfba4e35f313d1e610b571e408ef52a], retrying...."
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.689910147Z" level=info msg="Loading containers: done."
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.721867426Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.721995463Z" level=info msg="Initializing buildkit"
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.800946033Z" level=info msg="Completed buildkit initialization"
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.822418389Z" level=info msg="Daemon has completed initialization"
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.822587363Z" level=info msg="API listen on /run/docker.sock"
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.822758431Z" level=info msg="API listen on [::]:2376"
Nov 22 17:20:59 minikube dockerd[1140]: time="2025-11-22T17:20:59.822654407Z" level=info msg="API listen on /var/run/docker.sock"
Nov 22 17:20:59 minikube systemd[1]: Started Docker Application Container Engine.
Nov 22 17:21:00 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Start docker client with request timeout 0s"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Loaded network plugin cni"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Setting cgroupDriver systemd"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 22 17:21:00 minikube cri-dockerd[1449]: time="2025-11-22T17:21:00Z" level=info msg="Start cri-dockerd grpc backend"
Nov 22 17:21:00 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 22 17:21:08 minikube cri-dockerd[1449]: time="2025-11-22T17:21:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e953fd54a4dc3e5e46e908c78b8988f7002cbdd4338ae208ed7b8b458d80ebd6/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Nov 22 17:21:08 minikube cri-dockerd[1449]: time="2025-11-22T17:21:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cbb51038efdb6566556211cc342de7714601519a12e9d85e119105c883745835/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Nov 22 17:21:08 minikube cri-dockerd[1449]: time="2025-11-22T17:21:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68e63c14afa650df35a63329ae6a1587d88a7cf12a86c61826505799b9bbbc97/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Nov 22 17:21:08 minikube cri-dockerd[1449]: time="2025-11-22T17:21:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/55965ee4e25b39a9f15f8baff514edcf71c7dd28273b60f5d3bf9c2da6ec97c6/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 22 17:21:20 minikube cri-dockerd[1449]: time="2025-11-22T17:21:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb9e5a0d64c4f47e240a285e6439d57d927d68b9a9dc93690796b80088abb15c/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 22 17:21:20 minikube cri-dockerd[1449]: time="2025-11-22T17:21:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ac281db1e94c1fcacf1049e500f60efbe144f672a49e1675a823a559abed7da5/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 22 17:21:20 minikube cri-dockerd[1449]: time="2025-11-22T17:21:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/373846c94775c1749b45604391870e4047250ee8567d0b4346e98e670a1d2bc5/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 22 17:21:24 minikube cri-dockerd[1449]: time="2025-11-22T17:21:24Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 22 17:30:05 minikube cri-dockerd[1449]: time="2025-11-22T17:30:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/484446efc3a450b04a2f4c6d6b2fdf5412772726e2e5491f56c49e90aaf97164/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 22 17:30:05 minikube cri-dockerd[1449]: time="2025-11-22T17:30:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/46d782d9853d40afdb31baceb133a347237234fb514e2f6d5215ed1bb230ecc9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 22 17:30:05 minikube cri-dockerd[1449]: time="2025-11-22T17:30:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d22eb640509e9a8ddd6a69eee39cabc815e7b785d84c8823724e257060355462/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
f18c69ec7e846       6e38f40d628db       10 minutes ago      Running             storage-provisioner       0                   373846c94775c       storage-provisioner
b99377ba12234       52546a367cc9e       10 minutes ago      Running             coredns                   0                   ac281db1e94c1       coredns-66bc5c9577-cqlr4
5928569b76f5d       df0860106674d       10 minutes ago      Running             kube-proxy                0                   eb9e5a0d64c4f       kube-proxy-cdrnm
dc2233275e337       a0af72f2ec6d6       11 minutes ago      Running             kube-controller-manager   0                   55965ee4e25b3       kube-controller-manager-minikube
25b30b68eff78       46169d968e920       11 minutes ago      Running             kube-scheduler            0                   cbb51038efdb6       kube-scheduler-minikube
5176ad7326b46       5f1f5298c888d       11 minutes ago      Running             etcd                      0                   68e63c14afa65       etcd-minikube
0c42ba79f89d9       90550c43ad2bc       11 minutes ago      Running             kube-apiserver            0                   e953fd54a4dc3       kube-apiserver-minikube


==> coredns [b99377ba1223] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:51266 - 4586 "HINFO IN 542632899625335060.8372909779153925569. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.057395548s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_22T18_21_14_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 22 Nov 2025 17:21:10 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 22 Nov 2025 17:31:58 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 22 Nov 2025 17:28:01 +0000   Sat, 22 Nov 2025 17:21:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 22 Nov 2025 17:28:01 +0000   Sat, 22 Nov 2025 17:21:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 22 Nov 2025 17:28:01 +0000   Sat, 22 Nov 2025 17:21:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 22 Nov 2025 17:28:01 +0000   Sat, 22 Nov 2025 17:21:11 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16007836Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16007836Ki
  pods:               110
System Info:
  Machine ID:                 8a2cfd8707fa4c1a9d16827fb54dc57d
  System UUID:                94a843cb-d38a-4993-8c64-e8fce1df0332
  Boot ID:                    61939e7c-9370-4c4b-a77f-43863b61e34c
  Kernel Version:             6.8.0-85-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                       ------------  ----------  ---------------  -------------  ---
  default                     ml-modelops-deployment-557989b946-f6hgq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m4s
  default                     ml-modelops-deployment-557989b946-r9z74    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m4s
  default                     ml-modelops-deployment-557989b946-zhksr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m4s
  kube-system                 coredns-66bc5c9577-cqlr4                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     10m
  kube-system                 etcd-minikube                              100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         10m
  kube-system                 kube-apiserver-minikube                    250m (2%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-controller-manager-minikube           200m (1%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-proxy-cdrnm                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-scheduler-minikube                    100m (0%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 storage-provisioner                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 10m   kube-proxy       
  Normal  Starting                 10m   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  10m   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  10m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    10m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     10m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           10m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov22 15:00] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.311936] pnp 00:0b: disabling [mem 0x000c0000-0x000c3fff] because it overlaps 0000:00:02.0 BAR 6 [mem 0x000c0000-0x000dffff]
[  +0.000004] pnp 00:0b: disabling [mem 0x000c8000-0x000cbfff] because it overlaps 0000:00:02.0 BAR 6 [mem 0x000c0000-0x000dffff]
[  +0.000003] pnp 00:0b: disabling [mem 0x000d0000-0x000d3fff] because it overlaps 0000:00:02.0 BAR 6 [mem 0x000c0000-0x000dffff]
[  +0.000003] pnp 00:0b: disabling [mem 0x000d8000-0x000dbfff] because it overlaps 0000:00:02.0 BAR 6 [mem 0x000c0000-0x000dffff]
[  +0.036392] resource: resource sanity check: requesting [mem 0x00000000fed10000-0x00000000fed15fff], which spans more than pnp 00:08 [mem 0xfed10000-0xfed13fff]
[  +0.000004] caller snb_uncore_imc_init_box+0x7f/0xe0 mapping multiple BARs
[  +0.041216] hpet_acpi_add: no address or irqs in _CRS
[  +0.030192] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000115] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.253093] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.413591] nvme nvme0: missing or invalid SUBNQN field.
[  +1.518728] block nvme0n1: the capability attribute has been deprecated.
[  +0.029468] systemd[1]: Configuration file /run/systemd/system/netplan-ovs-cleanup.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
[  +1.086645] iwlwifi 0000:52:00.0: BIOS contains WGDS but no WRDS
[  +0.008875] thermal thermal_zone18: failed to read out thermal zone (-61)
[  +0.285433] thermal thermal_zone18: failed to read out thermal zone (-61)
[  +1.503468] Bluetooth: hci0: HCI LE Coded PHY feature bit is set, but its usage is not supported.
[  +0.186104] Bluetooth: hci0: Bad flag given (0x1) vs supported (0x0)
[  +5.000710] kauditd_printk_skb: 29 callbacks suppressed
[Nov22 15:01] workqueue: pm_runtime_work hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[ +41.943869] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[Nov22 15:36] workqueue: pm_runtime_work hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND
[  +0.253848] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=1 end=2) time 3639 us, min 1073, max 1079, scanline start 854, end 1098
[Nov22 15:42] i915 0000:00:02.0: [drm] *ERROR* CPU pipe A FIFO underrun
[Nov22 16:14] Bluetooth: hci0: Unknown advertising packet type: 0x40
[  +2.859962] Bluetooth: hci0: Unknown advertising packet type: 0x40
[  +2.574991] Bluetooth: hci0: Unknown advertising packet type: 0x40
[  +3.156008] Bluetooth: hci0: Unknown advertising packet type: 0x40
[  +2.846077] Bluetooth: hci0: Unknown advertising packet type: 0x40
[Nov22 16:15] Bluetooth: hci0: Unknown advertising packet type: 0x40
[  +0.284136] Bluetooth: hci0: Unknown advertising packet type: 0x40
[  +2.277866] Bluetooth: hci0: Unknown advertising packet type: 0x40
[Nov22 16:40] Bluetooth: hci0: Unknown advertising packet type: 0x40
[  +5.404110] Bluetooth: hci0: Unknown advertising packet type: 0x40


==> etcd [5176ad7326b4] <==
{"level":"warn","ts":"2025-11-22T17:21:09.722522Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39412","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.731306Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39430","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.739190Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39440","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.748124Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39458","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.757521Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39472","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.768753Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39490","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.776619Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39508","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.784802Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39522","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.792591Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39540","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.800233Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39564","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.808113Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39582","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.815755Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39612","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.823729Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39620","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.831952Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39650","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.839585Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39670","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.848550Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39676","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.856311Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39688","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.882916Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39710","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.895739Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39738","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.902108Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39766","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.908522Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39784","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.915002Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39822","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.923142Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39834","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.929765Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39842","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.937969Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39864","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.945411Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39880","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.953068Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39900","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.962837Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39926","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.969491Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39932","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.975976Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39946","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.982179Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39962","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.987980Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39980","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:09.993736Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39996","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.001137Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40020","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.009699Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40032","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.015340Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40054","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.021150Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40070","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.027693Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40092","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.034095Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40114","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.040512Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40134","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.047648Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40152","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.056423Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40166","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.063354Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40186","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.069952Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40202","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.077023Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40218","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.083857Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40242","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.090524Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40258","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.097111Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40266","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.103641Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40270","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.110037Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40290","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.116241Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40322","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.122702Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40352","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.129152Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40376","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.157276Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40396","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.166223Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40424","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.174870Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40454","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-22T17:21:10.222188Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40476","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-11-22T17:31:09.716681Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":615}
{"level":"info","ts":"2025-11-22T17:31:09.726613Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":615,"took":"9.281589ms","hash":2816105106,"current-db-size-bytes":1499136,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1499136,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-11-22T17:31:09.726691Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2816105106,"revision":615,"compact-revision":-1}


==> kernel <==
 17:32:08 up  2:31,  0 users,  load average: 0.91, 1.27, 1.07
Linux minikube 6.8.0-85-generic #85~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 19 16:18:59 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [0c42ba79f89d] <==
I1122 17:21:10.665584       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1122 17:21:10.665653       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1122 17:21:10.665605       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1122 17:21:10.665725       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1122 17:21:10.665795       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1122 17:21:10.665748       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I1122 17:21:10.665849       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1122 17:21:10.665917       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1122 17:21:10.665938       1 cache.go:39] Caches are synced for LocalAvailability controller
I1122 17:21:10.666128       1 aggregator.go:171] initial CRD sync complete...
I1122 17:21:10.666163       1 autoregister_controller.go:144] Starting autoregister controller
I1122 17:21:10.666184       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1122 17:21:10.665988       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1122 17:21:10.666210       1 cache.go:39] Caches are synced for autoregister controller
I1122 17:21:10.666443       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1122 17:21:10.666761       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1122 17:21:10.675240       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1122 17:21:10.675297       1 policy_source.go:240] refreshing policies
I1122 17:21:10.675475       1 controller.go:667] quota admission added evaluator for: namespaces
I1122 17:21:10.773689       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1122 17:21:10.774171       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1122 17:21:10.801757       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1122 17:21:10.802856       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1122 17:21:10.845619       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1122 17:21:11.584259       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1122 17:21:11.595486       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1122 17:21:11.595534       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1122 17:21:12.536276       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1122 17:21:12.613545       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1122 17:21:12.792673       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1122 17:21:12.807569       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1122 17:21:12.810603       1 controller.go:667] quota admission added evaluator for: endpoints
I1122 17:21:12.821484       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1122 17:21:13.588850       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1122 17:21:13.925718       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1122 17:21:13.953100       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1122 17:21:13.978552       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1122 17:21:19.407007       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1122 17:21:19.415658       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1122 17:21:19.449388       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1122 17:21:19.701949       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1122 17:22:09.901426       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:22:34.034662       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:23:32.987016       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:23:56.777297       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:24:45.351256       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:24:58.433226       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:25:47.539928       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:26:10.452472       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:26:59.820229       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:27:20.316490       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:28:18.861102       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:28:20.489186       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:29:30.897072       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:29:38.371224       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:30:21.682708       1 alloc.go:328] "allocated clusterIPs" service="default/ml-model-service" clusterIPs={"IPv4":"10.109.235.110"}
I1122 17:30:33.580378       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:30:47.697660       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1122 17:31:10.586861       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1122 17:31:50.518186       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [dc2233275e33] <==
I1122 17:21:18.507798       1 controllermanager.go:781] "Started controller" controller="resourcequota-controller"
I1122 17:21:18.507894       1 resource_quota_controller.go:300] "Starting resource quota controller" logger="resourcequota-controller"
I1122 17:21:18.507951       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1122 17:21:18.508142       1 resource_quota_monitor.go:308] "QuotaMonitor running" logger="resourcequota-controller"
I1122 17:21:18.527179       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1122 17:21:18.537948       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1122 17:21:18.537964       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1122 17:21:18.538416       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1122 17:21:18.542141       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1122 17:21:18.542152       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1122 17:21:18.542184       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1122 17:21:18.542526       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1122 17:21:18.542848       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1122 17:21:18.545606       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1122 17:21:18.548064       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1122 17:21:18.555065       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1122 17:21:18.587046       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1122 17:21:18.587421       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1122 17:21:18.587689       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1122 17:21:18.587878       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1122 17:21:18.589884       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1122 17:21:18.590209       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1122 17:21:18.590754       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1122 17:21:18.591567       1 shared_informer.go:356] "Caches are synced" controller="node"
I1122 17:21:18.591655       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1122 17:21:18.591665       1 shared_informer.go:356] "Caches are synced" controller="job"
I1122 17:21:18.591774       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1122 17:21:18.591722       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1122 17:21:18.591922       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1122 17:21:18.591966       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1122 17:21:18.591999       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1122 17:21:18.592600       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1122 17:21:18.592601       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1122 17:21:18.592716       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1122 17:21:18.592746       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1122 17:21:18.592997       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1122 17:21:18.594183       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1122 17:21:18.596383       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1122 17:21:18.603851       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1122 17:21:18.603931       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1122 17:21:18.603960       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1122 17:21:18.608878       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1122 17:21:18.609642       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1122 17:21:18.615411       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1122 17:21:18.623151       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1122 17:21:18.627417       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1122 17:21:18.629132       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1122 17:21:18.634724       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1122 17:21:18.638751       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1122 17:21:18.638877       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1122 17:21:18.640221       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1122 17:21:18.640350       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1122 17:21:18.641751       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1122 17:21:18.641990       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1122 17:21:18.642573       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1122 17:21:18.642635       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1122 17:21:18.643295       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1122 17:21:18.643408       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1122 17:21:18.644307       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1122 17:21:18.648983       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-proxy [5928569b76f5] <==
I1122 17:21:20.310634       1 server_linux.go:53] "Using iptables proxy"
I1122 17:21:20.394208       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1122 17:21:20.494986       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1122 17:21:20.495045       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1122 17:21:20.495199       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1122 17:21:20.568309       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1122 17:21:20.569049       1 server_linux.go:132] "Using iptables Proxier"
I1122 17:21:20.587687       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1122 17:21:20.594192       1 server.go:527] "Version info" version="v1.34.0"
I1122 17:21:20.594228       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1122 17:21:20.595963       1 config.go:403] "Starting serviceCIDR config controller"
I1122 17:21:20.595984       1 config.go:200] "Starting service config controller"
I1122 17:21:20.595989       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1122 17:21:20.595999       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1122 17:21:20.596039       1 config.go:106] "Starting endpoint slice config controller"
I1122 17:21:20.596060       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1122 17:21:20.596044       1 config.go:309] "Starting node config controller"
I1122 17:21:20.596083       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1122 17:21:20.596094       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1122 17:21:20.696969       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1122 17:21:20.697002       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1122 17:21:20.697013       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [25b30b68eff7] <==
I1122 17:21:09.247932       1 serving.go:386] Generated self-signed cert in-memory
W1122 17:21:10.589403       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1122 17:21:10.589439       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1122 17:21:10.589456       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1122 17:21:10.589470       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1122 17:21:10.609342       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1122 17:21:10.609360       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1122 17:21:10.611270       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1122 17:21:10.611298       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1122 17:21:10.612035       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1122 17:21:10.612237       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1122 17:21:10.613659       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1122 17:21:10.613680       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1122 17:21:10.614167       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1122 17:21:10.614264       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1122 17:21:10.614839       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1122 17:21:10.614924       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1122 17:21:10.614932       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1122 17:21:10.614948       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1122 17:21:10.614987       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1122 17:21:10.615042       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1122 17:21:10.615061       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1122 17:21:10.615063       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1122 17:21:10.615123       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1122 17:21:10.615141       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1122 17:21:10.615138       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1122 17:21:10.615206       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1122 17:21:10.615237       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1122 17:21:10.615267       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1122 17:21:10.615271       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1122 17:21:11.431258       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1122 17:21:11.454890       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1122 17:21:11.475875       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1122 17:21:11.548041       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1122 17:21:11.553220       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1122 17:21:11.607846       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1122 17:21:11.661759       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1122 17:21:11.672669       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1122 17:21:11.689536       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1122 17:21:11.735802       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1122 17:21:11.764310       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1122 17:21:11.895675       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1122 17:21:11.991126       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1122 17:21:12.034296       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1122 17:21:12.091138       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1122 17:21:12.123198       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1122 17:21:12.127116       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1122 17:21:12.135771       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1122 17:21:12.184574       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
I1122 17:21:14.512265       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Nov 22 17:30:05 minikube kubelet[2432]: E1122 17:30:05.614597    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:05 minikube kubelet[2432]: E1122 17:30:05.615050    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:30:05 minikube kubelet[2432]: E1122 17:30:05.634151    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:05 minikube kubelet[2432]: E1122 17:30:05.634368    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:30:05 minikube kubelet[2432]: E1122 17:30:05.652536    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:05 minikube kubelet[2432]: E1122 17:30:05.652588    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:30:16 minikube kubelet[2432]: E1122 17:30:16.785177    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:16 minikube kubelet[2432]: E1122 17:30:16.785281    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:30:17 minikube kubelet[2432]: E1122 17:30:17.786164    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:17 minikube kubelet[2432]: E1122 17:30:17.786292    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:30:18 minikube kubelet[2432]: E1122 17:30:18.783483    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:18 minikube kubelet[2432]: E1122 17:30:18.783520    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:30:29 minikube kubelet[2432]: E1122 17:30:29.785871    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:29 minikube kubelet[2432]: E1122 17:30:29.785940    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:29 minikube kubelet[2432]: E1122 17:30:29.786003    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:30:29 minikube kubelet[2432]: E1122 17:30:29.787133    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:30:32 minikube kubelet[2432]: E1122 17:30:32.785435    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:32 minikube kubelet[2432]: E1122 17:30:32.785563    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:30:41 minikube kubelet[2432]: E1122 17:30:41.786057    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:41 minikube kubelet[2432]: E1122 17:30:41.786200    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:30:43 minikube kubelet[2432]: E1122 17:30:43.787424    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:43 minikube kubelet[2432]: E1122 17:30:43.787512    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:30:44 minikube kubelet[2432]: E1122 17:30:44.785997    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:44 minikube kubelet[2432]: E1122 17:30:44.786145    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:30:55 minikube kubelet[2432]: E1122 17:30:55.784911    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:55 minikube kubelet[2432]: E1122 17:30:55.784973    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:55 minikube kubelet[2432]: E1122 17:30:55.785029    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:30:55 minikube kubelet[2432]: E1122 17:30:55.786251    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:30:57 minikube kubelet[2432]: E1122 17:30:57.786739    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:30:57 minikube kubelet[2432]: E1122 17:30:57.786880    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:31:06 minikube kubelet[2432]: E1122 17:31:06.786493    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:06 minikube kubelet[2432]: E1122 17:31:06.786606    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:31:08 minikube kubelet[2432]: E1122 17:31:08.785289    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:08 minikube kubelet[2432]: E1122 17:31:08.785461    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:08 minikube kubelet[2432]: E1122 17:31:08.785477    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:31:08 minikube kubelet[2432]: E1122 17:31:08.786731    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:31:19 minikube kubelet[2432]: E1122 17:31:19.784539    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:19 minikube kubelet[2432]: E1122 17:31:19.784584    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:31:19 minikube kubelet[2432]: E1122 17:31:19.784539    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:19 minikube kubelet[2432]: E1122 17:31:19.786295    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:31:23 minikube kubelet[2432]: E1122 17:31:23.786404    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:23 minikube kubelet[2432]: E1122 17:31:23.786463    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:31:31 minikube kubelet[2432]: E1122 17:31:31.802477    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:31 minikube kubelet[2432]: E1122 17:31:31.802589    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:31:32 minikube kubelet[2432]: E1122 17:31:32.785512    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:32 minikube kubelet[2432]: E1122 17:31:32.785640    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:31:35 minikube kubelet[2432]: E1122 17:31:35.785887    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:35 minikube kubelet[2432]: E1122 17:31:35.785985    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:31:43 minikube kubelet[2432]: E1122 17:31:43.801031    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:43 minikube kubelet[2432]: E1122 17:31:43.801167    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:31:46 minikube kubelet[2432]: E1122 17:31:46.786399    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:46 minikube kubelet[2432]: E1122 17:31:46.786510    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:31:50 minikube kubelet[2432]: E1122 17:31:50.785473    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:50 minikube kubelet[2432]: E1122 17:31:50.785583    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"
Nov 22 17:31:57 minikube kubelet[2432]: E1122 17:31:57.782986    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-zhksr_default(017c3d2d-a6b6-4446-97b2-2926b065991f): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:57 minikube kubelet[2432]: E1122 17:31:57.783036    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-zhksr" podUID="017c3d2d-a6b6-4446-97b2-2926b065991f"
Nov 22 17:31:57 minikube kubelet[2432]: E1122 17:31:57.783231    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-r9z74_default(c2130928-5072-4222-89b2-2fa354247419): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:31:57 minikube kubelet[2432]: E1122 17:31:57.784100    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-r9z74" podUID="c2130928-5072-4222-89b2-2fa354247419"
Nov 22 17:32:01 minikube kubelet[2432]: E1122 17:32:01.786034    2432 kuberuntime_manager.go:1449] "Unhandled Error" err="container ml-model-container start failed in pod ml-modelops-deployment-557989b946-f6hgq_default(f2dfe949-a0a3-4920-9954-b24d847110f0): ErrImageNeverPull: Container image \"mlops01:latest\" is not present with pull policy of Never" logger="UnhandledError"
Nov 22 17:32:01 minikube kubelet[2432]: E1122 17:32:01.786193    2432 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ml-model-container\" with ErrImageNeverPull: \"Container image \\\"mlops01:latest\\\" is not present with pull policy of Never\"" pod="default/ml-modelops-deployment-557989b946-f6hgq" podUID="f2dfe949-a0a3-4920-9954-b24d847110f0"


==> storage-provisioner [f18c69ec7e84] <==
W1122 17:31:08.533289       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:08.547633       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:10.553266       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:10.568020       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:12.572628       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:12.579838       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:14.585403       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:14.599742       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:16.607175       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:16.622074       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:18.627753       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:18.644376       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:20.649798       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:20.663969       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:22.666617       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:22.676690       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:24.682285       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:24.697045       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:26.704548       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:26.720007       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:28.723471       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:28.733290       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:30.738058       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:30.752358       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:32.757560       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:32.774072       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:34.780022       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:34.794215       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:36.800176       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:36.814177       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:38.819724       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:38.834546       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:40.840256       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:40.855591       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:42.861111       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:42.868348       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:44.873571       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:44.881884       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:46.889976       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:46.899410       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:48.905426       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:48.915230       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:50.921518       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:50.929458       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:52.935072       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:52.945789       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:54.951253       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:54.965257       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:56.970895       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:56.984743       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:58.990547       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:31:59.005391       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:01.010887       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:01.025649       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:03.031382       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:03.044992       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:05.051236       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:05.064893       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:07.068891       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1122 17:32:07.081609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

